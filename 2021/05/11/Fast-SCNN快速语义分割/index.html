<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Fast-SCNN快速语义分割 | ZeroRains Blog</title><meta name="author" content="zerorains,zerorainssakura@qq.com"><meta name="copyright" content="zerorains"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Fast-SCNN快速语义分割  论文名:Fast-SCNN: Fast Semantic Segmentation Network 作者：Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla 代码：无github代码，只有modelarts上的baseline  摘要 主要贡献：  提出了一个有竞争性(68.0%miou)，并且能在高分辨率(102">
<meta property="og:type" content="article">
<meta property="og:title" content="Fast-SCNN快速语义分割">
<meta property="og:url" content="http://blog.zerorains.top/2021/05/11/Fast-SCNN%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/index.html">
<meta property="og:site_name" content="ZeroRains Blog">
<meta property="og:description" content="Fast-SCNN快速语义分割  论文名:Fast-SCNN: Fast Semantic Segmentation Network 作者：Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla 代码：无github代码，只有modelarts上的baseline  摘要 主要贡献：  提出了一个有竞争性(68.0%miou)，并且能在高分辨率(102">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.zerorains.top/img/10.png">
<meta property="article:published_time" content="2021-05-11T06:42:30.000Z">
<meta property="article:modified_time" content="2022-06-30T08:17:58.117Z">
<meta property="article:author" content="zerorains">
<meta property="article:tag" content="语义分割">
<meta property="article:tag" content="实时检测">
<meta property="article:tag" content="高帧率">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.zerorains.top/img/10.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fast-SCNN快速语义分割",
  "url": "http://blog.zerorains.top/2021/05/11/Fast-SCNN%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/",
  "image": "http://blog.zerorains.top/img/10.png",
  "datePublished": "2021-05-11T06:42:30.000Z",
  "dateModified": "2022-06-30T08:17:58.117Z",
  "author": [
    {
      "@type": "Person",
      "name": "zerorains",
      "url": "http://blog.zerorains.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/favicon.ico"><link rel="canonical" href="http://blog.zerorains.top/2021/05/11/Fast-SCNN%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Fast-SCNN快速语义分割',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg" style="background-image: url(/img/body_background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/apple-touch-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">104</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/drink/"><i class="fa-fw fas fa-mug-hot"></i><span> 请我喝茶</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://ml.akasaki.space/"><i class="fa-fw fas fa-link"></i><span> DL笔记</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://notebook.therainisme.com/"><i class="fa-fw fas fa-link"></i><span> 急救箱</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/10.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ZeroRains Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Fast-SCNN快速语义分割</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/drink/"><i class="fa-fw fas fa-mug-hot"></i><span> 请我喝茶</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://ml.akasaki.space/"><i class="fa-fw fas fa-link"></i><span> DL笔记</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://notebook.therainisme.com/"><i class="fa-fw fas fa-link"></i><span> 急救箱</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Fast-SCNN快速语义分割</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-05-11T06:42:30.000Z" title="发表于 2021-05-11 14:42:30">2021-05-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-06-30T08:17:58.117Z" title="更新于 2022-06-30 16:17:58">2022-06-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="fast-scnn快速语义分割">Fast-SCNN快速语义分割</h1>
<blockquote>
<p>论文名:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.04502">Fast-SCNN: Fast Semantic Segmentation Network</a></p>
<p>作者：<a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Poudel%2C+R+P+K">Rudra P K Poudel</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liwicki%2C+S">Stephan Liwicki</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cipolla%2C+R">Roberto Cipolla</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://bbs.huaweicloud.com/forum/thread-119511-1-1.html">无github代码，只有modelarts上的baseline</a></p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>主要贡献：</p>
<ol>
<li>提出了一个有竞争性(68.0%miou)，并且能在高分辨率(1024x2048)的图片实现实时(123.5FPS)语义分割的算法Fast-SCNN.</li>
<li>采用了离线型DCNNs中流行的<strong>跳跃连接(skip connection)</strong>，并提出了一种浅层学习的下采样模块<strong>learning to Down-sample</strong>,以此更加快速高效地进行多分支低级特征提取。</li>
<li>将Fast-SCNN设计为轻量型(low capacity)，并证实了无论是使用ImageNet数据集的训练模型多训练几代，还是在添加的粗糙数据中多训练几代的结果是等效的。</li>
</ol>
<h2 id="dcnns的效率">DCNNs的效率</h2>
<p>高效DCNNs（Diffusion-Convolutional Neural Networks ）的常见技术为：</p>
<h3 id="深度可分离卷积-depthwise-separable-convolutions">深度可分离卷积(Depthwise Separable Convolutions):</h3>
<p>MoblieNet将标准的Conv分解为**深度卷积（depthwise convolutions）**和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>**点式卷积(pointwise convolution)**通过这样的方式，减少了浮点运算和卷积参数，减少了模型的计算成本和内存需求。</p>
<h3 id="dcnns的高效重新设计">DCNNs的高效重新设计</h3>
<p>MobileNet-V2使用**倒置的瓶颈残差块(inverted bottleneck residual blocks)**以分类任务构建有效的DCNN。</p>
<p>ContextNeto使用能够倒置瓶颈残差块设计了一个1两分支网络，以进行有效的实时语义分割。</p>
<h3 id="网络压缩">网络压缩</h3>
<p>使用剪枝减小预训练网络的大小，从而实现更块的运行时间，更小的参数集和更小的内存占用空间。</p>
<p>Fast-SCNN严重依赖与深度可分离卷积和残差瓶颈块，还引入了一个两分支模型，该模型将学习内容整合到下采样的模块中，从而允许在多个分辨率级别上进行共享特征提取。网络量化和网络压缩可以正交应用，留待后面的工作。</p>
<h2 id="fast-scnn">Fast-SCNN</h2>
<p>网络结构图：</p>
<p><img src="https://blog.zerorains.top/img/20210512115432image-20210512115431363.png" alt="image-20210512115431363"></p>
<p>在定义网络的BN层时使用类各种类型的BN层,但是默认都是使用普通的BN层</p>
<p>常规的BN，SyncBN（跨卡BN），FrozenBN（测试阶段使用的BN），GN（Group Normalization）</p>
<p><img src="https://blog.zerorains.top/img/20210516102348image-20210516102346560.png" alt="image-20210516102346560"></p>
<h3 id="下采样学习模块-learning-to-down-sample">下采样学习模块(learning to down-sample)</h3>
<p>在该模块中使用了三层卷积，第一层是普通的卷积(Conv2D)，其余两层是可分离卷积(DSConv)，因为图像刚刚输入只有三个通道，使用DSConv的优势并不明显所以，采用普通卷积层。</p>
<p>在下采样学习模块中，使用的步长均为2，然后进行BN和ReLU。卷积核和深度可分离卷积核均为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在主网络中的定义</span></span><br><span class="line"><span class="variable language_">self</span>.learning_to_downsample = LearningToDownsample(<span class="number">32</span>, <span class="number">48</span>, <span class="number">64</span>, norm_layer=<span class="variable language_">self</span>.norm_layer) <span class="comment"># norm_layerh是普通的BN</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下采样学习模块的定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearningToDownsample</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Learning to downsample module&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dw_channels1=<span class="number">32</span>, dw_channels2=<span class="number">48</span>, out_channels=<span class="number">64</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(LearningToDownsample, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = _ConvBNReLU(<span class="number">3</span>, dw_channels1, <span class="number">3</span>, <span class="number">2</span>) <span class="comment"># 这个就是单纯的CONV+BN+ReLU</span></span><br><span class="line">        <span class="comment"># 深度可分离卷积：一个深度卷积，一个点卷积的组合</span></span><br><span class="line">        <span class="variable language_">self</span>.dsconv1 = SeparableConv2d(dw_channels1, dw_channels2, stride=<span class="number">2</span>, relu_first=<span class="literal">False</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="variable language_">self</span>.dsconv2 = SeparableConv2d(dw_channels2, out_channels, stride=<span class="number">2</span>, relu_first=<span class="literal">False</span>, norm_layer=norm_layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x) <span class="comment">#  普通卷积</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dsconv1(x) <span class="comment"># 可分离卷积</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dsconv2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 深度可分离卷积</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SeparableConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, relu_first=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 bias=<span class="literal">False</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 深度卷积，卷积核为3步长1，padding1，空洞1的卷积层</span></span><br><span class="line">        depthwise = nn.Conv2d(inplanes, inplanes, kernel_size,</span><br><span class="line">                              stride=stride, padding=dilation,</span><br><span class="line">                              dilation=dilation, groups=inplanes, bias=bias)</span><br><span class="line">        <span class="comment"># 对应的BN</span></span><br><span class="line">        bn_depth = norm_layer(inplanes)</span><br><span class="line">        <span class="comment"># 点卷积，就是普通的1x1卷积</span></span><br><span class="line">        pointwise = nn.Conv2d(inplanes, planes, <span class="number">1</span>, bias=bias)</span><br><span class="line">        <span class="comment"># 对应的BN</span></span><br><span class="line">        bn_point = norm_layer(planes)</span><br><span class="line">        <span class="comment"># 是否使用激活函数</span></span><br><span class="line">        <span class="keyword">if</span> relu_first:</span><br><span class="line">            <span class="variable language_">self</span>.block = nn.Sequential(OrderedDict([(<span class="string">&#x27;relu&#x27;</span>, nn.ReLU()),</span><br><span class="line">                                                    (<span class="string">&#x27;depthwise&#x27;</span>, depthwise),</span><br><span class="line">                                                    (<span class="string">&#x27;bn_depth&#x27;</span>, bn_depth),</span><br><span class="line">                                                    (<span class="string">&#x27;pointwise&#x27;</span>, pointwise),</span><br><span class="line">                                                    (<span class="string">&#x27;bn_point&#x27;</span>, bn_point)</span><br><span class="line">                                                    ]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.block = nn.Sequential(OrderedDict([(<span class="string">&#x27;depthwise&#x27;</span>, depthwise),</span><br><span class="line">                                                    (<span class="string">&#x27;bn_depth&#x27;</span>, bn_depth),</span><br><span class="line">                                                    (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU(inplace=<span class="literal">True</span>)),</span><br><span class="line">                                                    (<span class="string">&#x27;pointwise&#x27;</span>, pointwise),</span><br><span class="line">                                                    (<span class="string">&#x27;bn_point&#x27;</span>, bn_point),</span><br><span class="line">                                                    (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line">                                                    ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.block(x)</span><br></pre></td></tr></table></figure>
<h3 id="全局特征提取器-global-feature-extrator">全局特征提取器(Global Feature Extrator)</h3>
<p>全局特征提取器模块的目的在于捕获分割图像的全局上下文信息。该模块直接将下采样学习模块的结果(分辨率为原图的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac 18</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>)作为而输入。该模块引入了MobileNet-V2中提出的有效的瓶<strong>颈残差网络(efficient bottleneck residual blocks)</strong>。当输入的图像和输出的图像尺寸相同时，使用残差连接链接瓶颈残差块。</p>
<p>在瓶颈残差块中使用了有效的深度可分离卷积，从而减少了参数量和浮点数运算。最后还添加了一个金字塔池化模块(pyramid pooling module 简称PPM)，用于汇总基于不同区域的上下文信息。</p>
<p>在各层的详细参数如下表：</p>
<p><img src="https://blog.zerorains.top/img/20210512135254image-20210512135253006.png" alt="image-20210512135253006"></p>
<p>每一条横线分别表示，下采样学习模块，全局特征提取器，特征融合，分类四个总体模块</p>
<p>其中t,c,n,s分别表示瓶颈块的拓展因子，输入通道数，使用该层的次数，步长</p>
<p>瓶颈块的参数表：</p>
<p><img src="https://blog.zerorains.top/img/20210512135639image-20210512135638073.png" alt="image-20210512135638073"></p>
<p>瓶颈残差块将输入为c的图像转化为具有拓展因子t的c`</p>
<p>最后的点卷积不适用非线性函数f</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主网络声明</span></span><br><span class="line"><span class="variable language_">self</span>.global_feature_extractor = GlobalFeatureExtractor(<span class="number">64</span>, [<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>], <span class="number">128</span>, <span class="number">6</span>, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],norm_layer=<span class="variable language_">self</span>.norm_layer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局特征提取器对应的模块类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GlobalFeatureExtractor</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Global feature extractor module&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入的通道数，每一层的通道数，输出的通道数，拓展因子t，块在每一层的数量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels=<span class="number">64</span>, block_channels=(<span class="params"><span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span></span>), out_channels=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 t=<span class="number">6</span>, num_blocks=(<span class="params"><span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span></span>), norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(GlobalFeatureExtractor, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 创建瓶颈残差块，这里使用的InvertedResidual叫做反向残差。</span></span><br><span class="line">        <span class="comment"># 只有步长为1并且输入通道和输出通道相同的情况下这各个反向残差才会使用残差连接</span></span><br><span class="line">        <span class="variable language_">self</span>.bottleneck1 = <span class="variable language_">self</span>._make_layer(InvertedResidual, in_channels, block_channels[<span class="number">0</span>], num_blocks[<span class="number">0</span>],</span><br><span class="line">                                            t, <span class="number">2</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="variable language_">self</span>.bottleneck2 = <span class="variable language_">self</span>._make_layer(InvertedResidual, block_channels[<span class="number">0</span>], block_channels[<span class="number">1</span>],</span><br><span class="line">                                            num_blocks[<span class="number">1</span>], t, <span class="number">2</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="variable language_">self</span>.bottleneck3 = <span class="variable language_">self</span>._make_layer(InvertedResidual, block_channels[<span class="number">1</span>], block_channels[<span class="number">2</span>],</span><br><span class="line">                                            num_blocks[<span class="number">2</span>], t, <span class="number">1</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="comment"># 做一个金字塔池化</span></span><br><span class="line">        <span class="variable language_">self</span>.ppm = PyramidPooling(block_channels[<span class="number">2</span>], norm_layer=norm_layer)</span><br><span class="line">        <span class="comment"># 最后使用1x1卷积输出成对应的通道，进行输出</span></span><br><span class="line">        <span class="variable language_">self</span>.out = _ConvBNReLU(block_channels[<span class="number">2</span>] * <span class="number">2</span>, out_channels, <span class="number">1</span>, norm_layer=norm_layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, inplanes, planes, blocks, t=<span class="number">6</span>, stride=<span class="number">1</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="comment"># 使用的模块，输入的通道数，输出的通道数，块的数量，拓展因子t，步长</span></span><br><span class="line">        <span class="comment"># 初始化一个容器</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="comment"># 将块中的信息加入</span></span><br><span class="line">        layers.append(block(inplanes, planes, stride, t, norm_layer=norm_layer))</span><br><span class="line">        <span class="comment"># 重复这个块对应次</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(block(planes, planes, <span class="number">1</span>, t, norm_layer=norm_layer))</span><br><span class="line">        <span class="comment">#  将对应的内容放入Sequential容器中</span></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.bottleneck1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bottleneck2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bottleneck3(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.ppm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向卷积块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride, expand_ratio, dilation=<span class="number">1</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="comment"># 参数：输入通道，输出通道，步长，拓展因子，空洞卷积，</span></span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> stride <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="comment"># 是否使用残差连接</span></span><br><span class="line">        <span class="variable language_">self</span>.use_res_connect = stride == <span class="number">1</span> <span class="keyword">and</span> in_channels == out_channels</span><br><span class="line"></span><br><span class="line">        layers = <span class="built_in">list</span>()</span><br><span class="line">        <span class="comment"># 中间的通道数，使用拓展因子*输入的通道数</span></span><br><span class="line">        inter_channels = <span class="built_in">int</span>(<span class="built_in">round</span>(in_channels * expand_ratio))</span><br><span class="line">        <span class="keyword">if</span> expand_ratio != <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># pw</span></span><br><span class="line">            <span class="comment"># 先做一个标准卷积嘛，使用中间通道数作为输出,1x1卷积</span></span><br><span class="line">            layers.append(_ConvBNReLU(in_channels, inter_channels, <span class="number">1</span>, relu6=<span class="literal">True</span>, norm_layer=norm_layer))</span><br><span class="line">        layers.extend([</span><br><span class="line">            <span class="comment"># dw 这里使用了分组卷积，但是实际上和普通的卷积没有什么区别，如果groups整好是输入通道数的一个因素，则输入的通道会被分成对应的组进行卷积</span></span><br><span class="line">            _ConvBNReLU(inter_channels, inter_channels, <span class="number">3</span>, stride, dilation, dilation,</span><br><span class="line">                        groups=inter_channels, relu6=<span class="literal">True</span>, norm_layer=norm_layer),</span><br><span class="line">            <span class="comment"># pw-linear</span></span><br><span class="line">            <span class="comment"># 使用1x1卷积将中间通道数转化成最终的通道数</span></span><br><span class="line">            nn.Conv2d(inter_channels, out_channels, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            norm_layer(out_channels)])</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_res_connect:</span><br><span class="line">            <span class="keyword">return</span> x + <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.conv(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PPM(金字塔池化模块)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PyramidPooling</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, sizes=(<span class="params"><span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span></span>), norm_layer=nn.BatchNorm2d, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PyramidPooling, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义输出为输入的四分之一</span></span><br><span class="line">        out_channels = <span class="built_in">int</span>(in_channels / <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 创建平均池化和卷积层模块列表</span></span><br><span class="line">        <span class="variable language_">self</span>.avgpools = nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.convs = nn.ModuleList()</span><br><span class="line">        <span class="comment"># 遍历平均池化的尺寸</span></span><br><span class="line">        <span class="keyword">for</span> size <span class="keyword">in</span> sizes:</span><br><span class="line">            <span class="comment"># 使用自适应平均池化，这里的参数，表示经过自适应平均池化的特征图输入为c X h X w，出来的结果为c X size X size</span></span><br><span class="line">            <span class="variable language_">self</span>.avgpools.append(nn.AdaptiveAvgPool2d(size))</span><br><span class="line">            <span class="comment"># 使用普通卷积层进行卷积1x1的卷积核</span></span><br><span class="line">            <span class="variable language_">self</span>.convs.append(_ConvBNReLU(in_channels, out_channels, <span class="number">1</span>, norm_layer=norm_layer, **kwargs))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">2</span>:]</span><br><span class="line">        feats = [x]</span><br><span class="line">        <span class="keyword">for</span> (avgpool, conv) <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.avgpools, <span class="variable language_">self</span>.convs):</span><br><span class="line">            <span class="comment"># 没记错的话interpolate应该是上采样到size的大小</span></span><br><span class="line">            feats.append(F.interpolate(conv(avgpool(x)), size, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>))</span><br><span class="line">        <span class="comment"># 记录完平均池化的结果后，就进行拼接</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(feats, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="特征融合模块-feature-fusion-module">特征融合模块(Feature Fusion Module)</h3>
<p><img src="https://blog.zerorains.top/img/20210512140504image-20210512140501709.png" alt="image-20210512140501709"></p>
<p>先前下采样学习模块计算的特征图（表的左边）只经过一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>的卷积即可，在x次下采样后的结果(经过全局特征提取模块的特征图，表的右边)，上采样X次，使用可分离卷积和一个非线性函数，再使用一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>的卷积，最后将两个特征图加起来，再使用非线性激活函数f</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在主类中的声明：</span></span><br><span class="line"><span class="variable language_">self</span>.feature_fusion = FeatureFusionModule(<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>, norm_layer=<span class="variable language_">self</span>.norm_layer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征融合模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeatureFusionModule</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature fusion module&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入的参数为高输入的通道数，低输入的通道数，输出的通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, highter_in_channels, lower_in_channels, out_channels, scale_factor=<span class="number">4</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeatureFusionModule, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 设置规模</span></span><br><span class="line">        <span class="variable language_">self</span>.scale_factor = scale_factor</span><br><span class="line">        <span class="comment"># 使用普通卷积将低通道数转化成输出的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dwconv = _ConvBNReLU(lower_in_channels, out_channels, <span class="number">1</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="comment"># 再对低维卷积的将诶过再做一个1x1卷积，但是不激活</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_lower_res = nn.Sequential(</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, <span class="number">1</span>),</span><br><span class="line">            norm_layer(out_channels)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 对高维度的卷积，只使用1x1卷积，不使用激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_higher_res = nn.Sequential(</span><br><span class="line">            nn.Conv2d(highter_in_channels, out_channels, <span class="number">1</span>),</span><br><span class="line">            norm_layer(out_channels)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, higher_res_feature, lower_res_feature</span>):</span><br><span class="line">        <span class="comment"># 先将低维特征图上采样到现在的4倍</span></span><br><span class="line">        lower_res_feature = F.interpolate(lower_res_feature, scale_factor=<span class="number">4</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 将低纬度的通道数转化成输出的通道数</span></span><br><span class="line">        lower_res_feature = <span class="variable language_">self</span>.dwconv(lower_res_feature)</span><br><span class="line">        <span class="comment"># 再做一次1x1卷积，但是不激活</span></span><br><span class="line">        lower_res_feature = <span class="variable language_">self</span>.conv_lower_res(lower_res_feature)</span><br><span class="line">        <span class="comment"># 对高纬度进行1x1卷积，但是不激活</span></span><br><span class="line">        higher_res_feature = <span class="variable language_">self</span>.conv_higher_res(higher_res_feature)</span><br><span class="line">        <span class="comment"># 将低纬度和高纬度加起来</span></span><br><span class="line">        out = higher_res_feature + lower_res_feature</span><br><span class="line">        <span class="comment"># 最后激活他就行</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(out)</span><br></pre></td></tr></table></figure>
<h3 id="分类模块-classifier">分类模块(classifier)</h3>
<p>在分类模块中采用两个深度可分离卷积(DSConv)和一个普通卷积(Conv2D，纠正一下，之前说过的点卷积是Conv2D)。</p>
<p>为了适应梯度下降，所以在训练中使用了Softmax激活函数，在推理过程中,由于argmax和sorftmax都是单调递增的函数，所以使用argmax代替softmax减小计算开销。</p>
<p>如果需要Fast-SCNN的概率模型，才在推理时使用softmax。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在主类中的声明：</span></span><br><span class="line"><span class="variable language_">self</span>.classifier = Classifer(<span class="number">128</span>, <span class="variable language_">self</span>.nclass, norm_layer=<span class="variable language_">self</span>.norm_layer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Classifer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Classifer&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dw_channels, num_classes, stride=<span class="number">1</span>, norm_layer=nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="comment"># 参数：输入的通道数，分类数，步长，BN</span></span><br><span class="line">        <span class="built_in">super</span>(Classifer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 使用2个深度分离卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.dsconv1 = SeparableConv2d(dw_channels, dw_channels, stride=stride, relu_first=<span class="literal">False</span>,</span><br><span class="line">                                       norm_layer=norm_layer)</span><br><span class="line">        <span class="variable language_">self</span>.dsconv2 = SeparableConv2d(dw_channels, dw_channels, stride=stride, relu_first=<span class="literal">False</span>,</span><br><span class="line">                                       norm_layer=norm_layer)</span><br><span class="line">        <span class="comment"># 设置随机失活(dropout2d)，然后进行卷积，不适用BN不使用，激活，使用1x1卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Sequential(</span><br><span class="line">            nn.Dropout2d(<span class="number">0.1</span>),</span><br><span class="line">            nn.Conv2d(dw_channels, num_classes, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.dsconv1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dsconv2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://blog.zerorains.top">zerorains</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://blog.zerorains.top/2021/05/11/Fast-SCNN%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">http://blog.zerorains.top/2021/05/11/Fast-SCNN%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://blog.zerorains.top" target="_blank">ZeroRains Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">语义分割</a><a class="post-meta__tags" href="/tags/%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B/">实时检测</a><a class="post-meta__tags" href="/tags/%E9%AB%98%E5%B8%A7%E7%8E%87/">高帧率</a></div><div class="post-share"><div class="social-share" data-image="/img/10.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2021/05/11/%E6%9C%80%E7%9F%AD%E8%B7%AF%E9%97%AE%E9%A2%98/" title="最短路问题"><img class="cover" src="/img/12.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">最短路问题</div></div><div class="info-2"><div class="info-item-1">最短路问题的类型与解决算法 最短路问题分为单源最短路(1-n的最短路)和多元汇最短路(源点=起点，汇点=终点)   单元最短路  所有边权都是正数  朴素Dijkstra算法(O(n2)O(n^2)O(n2))，适合稠密图 堆优化的Dijkstra算法(O(mlog⁡n)O(m\log n)O(mlogn))   存在负边权  Bellman-Ford (O(nm)O(nm)O(nm)) SPFA  一般：(O(m)O(m)O(m))，最坏(O(nm)O(nm)O(nm))      多元汇最短路 Floyd算法 O(n3)O(n^3)O(n3)   单元最短路 朴素Dijkstra算法 先加入起点 起点到其他点的距离都为无穷大，到自己是0 更新起点到其他点的距离，更新时判断是否最短 从未选择的点中选择距离最小的点，加入点集，重复上述操作，直至所有点都进入点集 算法实现 12345678910111213141516171819202122232425262728293031323334353637383940#include...</div></div></div></a><a class="pagination-related" href="/2021/05/12/%E4%B8%BA%E8%BF%90%E7%AE%97%E8%BF%90%E7%94%A8/" title="位运算运用"><img class="cover" src="/img/27.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">位运算运用</div></div><div class="info-2"><div class="info-item-1">位运算的应用 快速幂 题目： a^b 快速幂讲解 假设我们要计算3113^{11}311 这个当然可以用暴力的方式去算，没啥问题，那这样就要算11次，即n次 可以看一下11的二进制为1011 311=38∗32∗313^{11} = 3^8*3^2*3^1311=38∗32∗31 这个结果很显然，可以得出11的二进制中对应1的位置，恰好为3113^{11}311的一个因素，这是不是巧合呢？我们不妨再尝试一个 30的二进制为11110 1030=1016∗108∗104∗10210^{30} = 10^{16}*10^{8}*10^4*10^21030=1016∗108∗104∗102 显然这不是一个巧合。 而我们可以看出，二进制指数的计算过层是可以被简化的 比如1016=28∗2810^{16} = 2^{8}*2^{8}1016=28∗28，在前面的计算过程中，我们又可以通过242^424计算出282^828。 这就是快速幂的思想了 AC代码： 123456789101112131415161718192021222324252627#include...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2021/05/17/BiSeNet%E5%AE%9E%E6%97%B6%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" title="BiSeNet实时语义分割"><img class="cover" src="/img/9.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-17</div><div class="info-item-2">BiSeNet实时语义分割</div></div><div class="info-2"><div class="info-item-1">BiSeNet实时语义分割  论文名称：BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation 作者：Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang 期刊：ECCV...</div></div></div></a><a class="pagination-related" href="/2021/05/05/%E5%AE%9E%E6%97%B6%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" title="实时语义分割"><img class="cover" src="/img/16.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-05</div><div class="info-item-2">实时语义分割</div></div><div class="info-2"><div class="info-item-1"> 论文名称：Rethinking BiSeNet For Real-time Semantic Segmentation 作者：Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, Xiaolin Wei code：https://github.com/MichaelFan01/STDC-Seg（尚未开源）  摘要 BiSeNet是目前流行的实时分割两流网络，但是添加额外的路径去进行编码非常耗时，并且由于任务专用设计的不足，主干可能无法有效地进行图像分割。 本文提出了STDC网络，通过消除结构冗余来实短期密集级联网络。 逐渐减小特征图维度，并将他们聚集用于图像表示，这构成了STDC网络的基本模块。在解码器中，我们通过将空间信息的学习以单流的方式集成到底层中，从而提出了一个细节集合模块。最后将底层特征和深层特征融合在一起，以预测最终的分割结果。 Short-Term Dense Concatenate Module(STDC，短期密集级联模块)  ConvX ​   ...</div></div></div></a><a class="pagination-related" href="/2021/09/01/Global-Aggregation-then-Local-Distribution-in-Fully-Convolutional-Networks/" title="Global Aggregation then Local Distribution in Fully Convolutional Networks"><img class="cover" src="/img/21.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-01</div><div class="info-item-2">Global Aggregation then Local Distribution in Fully Convolutional Networks</div></div><div class="info-2"><div class="info-item-1">Global Aggregation then Local Distribution in Fully Convolutional Networks  论文：Global Aggregation then Local Distribution in Fully Convolutional Networks 作者：Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong 期刊：BMVC 2019 代码：https://github.com/lxtGH/GALD-DGCNet  原文摘要  It has been widely proven that modelling long-range dependencies in fully convolutionalnetworks (FCNs) via global aggregation modules is critical for complex scene under-standing  tasks  such  as  semantic...</div></div></div></a><a class="pagination-related" href="/2021/09/03/FaPN-Feature-aligned-Pyramid-Network-for-Dense-Image-Prediction/" title="FaPN-Feature-aligned Pyramid Network for Dense Image Prediction"><img class="cover" src="/img/20210505093832image-20210505093830789.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-03</div><div class="info-item-2">FaPN-Feature-aligned Pyramid Network for Dense Image Prediction</div></div><div class="info-2"><div class="info-item-1">FaPN: Feature-aligned Pyramid Network for Dense Image Prediction  论文：FaPN: Feature-aligned Pyramid Network for Dense Image Prediction 作者：Shihua Huang, Zhichao Lu, Ran Cheng, Cheng He 期刊： ICCV2021 代码：https://github.com/EMI-Group/FaPN  原文摘要  Recent  advancements  in  deep  neural  networks  havemade remarkable leap-forwards in dense image prediction.However,  the  issue  of  feature  alignment  remains  as  ne-glected by most existing approaches for simplicity.  Directpixel addition between...</div></div></div></a><a class="pagination-related" href="/2021/08/18/Object-Contextual-Representations-for-Semantic-Segmentation/" title="Object-Contextual Representations for Semantic Segmentation"><img class="cover" src="/img/32.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-18</div><div class="info-item-2">Object-Contextual Representations for Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1">Object-Contextual Representations for Semantic Segmentation  论文名称：Object-Contextual Representations for Semantic Segmentation 作者：Yuhui Yuan, Xilin Chen, Jingdong Wang 期刊：ECCV2020 代码：https://github.com/HRNet/HRNet-Semantic-Segmentation  原文摘要  In this paper, we study the context aggregation problem in semantic  segmentation.  Motivated  by  that  the  label  of  a  pixel  is  the category  of  the  object  that  the  pixel  belongs  to,  we  present  a  simple yet  effective  approach, ...</div></div></div></a><a class="pagination-related" href="/2021/08/17/Hierarchical-Multi-Scale-Attention-for-Semantic-Segmentation/" title="Hierarchical Multi-Scale Attention for Semantic Segmentation"><img class="cover" src="/img/2.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="info-item-2">Hierarchical Multi-Scale Attention for Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1">Hierarchical Multi-Scale Attention for Semantic Segmentation  论文名称：Hierarchical Multi-Scale Attention for Semantic Segmentation 作者：Andrew Tao, Karan Sapra, Bryan Catanzaro 期刊：尚未查出（时间2020） 代码：https://github.com/NVIDIA/semantic-segmentation  原文摘要  Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/apple-touch-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zerorains</div><div class="author-info-description">No matter what happens, I will do my best.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">104</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zeroRains"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="tencent://message?uin=2274033547" target="_blank" title="qq"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://github.com/zeroRains" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zerorainssakura@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/kiminoamae?spm=1000.2115.3001.5343" target="_blank" title="csdn"><i class="fab fa-cuttlefish"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">主业想做大模型推理，目前也正在努力学习中。副业做数据库中执行传统模型的推理优化。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#fast-scnn%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-number">1.</span> <span class="toc-text">Fast-SCNN快速语义分割</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dcnns%E7%9A%84%E6%95%88%E7%8E%87"><span class="toc-number">1.2.</span> <span class="toc-text">DCNNs的效率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF-depthwise-separable-convolutions"><span class="toc-number">1.2.1.</span> <span class="toc-text">深度可分离卷积(Depthwise Separable Convolutions):</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dcnns%E7%9A%84%E9%AB%98%E6%95%88%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.2.2.</span> <span class="toc-text">DCNNs的高效重新设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9"><span class="toc-number">1.2.3.</span> <span class="toc-text">网络压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fast-scnn"><span class="toc-number">1.3.</span> <span class="toc-text">Fast-SCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E9%87%87%E6%A0%B7%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9D%97-learning-to-down-sample"><span class="toc-number">1.3.1.</span> <span class="toc-text">下采样学习模块(learning to down-sample)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8-global-feature-extrator"><span class="toc-number">1.3.2.</span> <span class="toc-text">全局特征提取器(Global Feature Extrator)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9D%97-feature-fusion-module"><span class="toc-number">1.3.3.</span> <span class="toc-text">特征融合模块(Feature Fusion Module)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9D%97-classifier"><span class="toc-number">1.3.4.</span> <span class="toc-text">分类模块(classifier)</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/02/%E3%80%8C%E9%9A%8F%E7%AC%94%E5%B0%8F%E8%AE%B0%E3%80%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9A%8F%E7%AC%94/" title="「随笔小记」大模型随笔"><img src="/img/27.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「随笔小记」大模型随笔"/></a><div class="content"><a class="title" href="/2025/03/02/%E3%80%8C%E9%9A%8F%E7%AC%94%E5%B0%8F%E8%AE%B0%E3%80%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9A%8F%E7%AC%94/" title="「随笔小记」大模型随笔">「随笔小记」大模型随笔</a><time datetime="2025-03-02T04:54:32.000Z" title="发表于 2025-03-02 12:54:32">2025-03-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/23/%E3%80%8C%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%8DPaddle%E7%BB%84%E5%90%88%E6%9C%BA%E5%88%B6%E8%AE%BE%E8%AE%A1/" title="「实习笔记」Paddle组合机制设计与开发"><img src="/img/2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「实习笔记」Paddle组合机制设计与开发"/></a><div class="content"><a class="title" href="/2025/02/23/%E3%80%8C%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%8DPaddle%E7%BB%84%E5%90%88%E6%9C%BA%E5%88%B6%E8%AE%BE%E8%AE%A1/" title="「实习笔记」Paddle组合机制设计与开发">「实习笔记」Paddle组合机制设计与开发</a><time datetime="2025-02-23T07:54:26.000Z" title="发表于 2025-02-23 15:54:26">2025-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/26/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DPiPAD-Pipelined-and-Parallel-Dynamic-GNN-Training-on-GPUs/" title="「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"><img src="/img/22.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"/></a><div class="content"><a class="title" href="/2023/04/26/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DPiPAD-Pipelined-and-Parallel-Dynamic-GNN-Training-on-GPUs/" title="「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs">「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs</a><time datetime="2023-04-26T05:34:14.000Z" title="发表于 2023-04-26 13:34:14">2023-04-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/04/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DEkko-A-Large-Scale-Deep-Learning-Recommender-System-with-Low-Latency-Model-Update/" title="「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update"><img src="/img/26.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update"/></a><div class="content"><a class="title" href="/2023/04/04/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DEkko-A-Large-Scale-Deep-Learning-Recommender-System-with-Low-Latency-Model-Update/" title="「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update">「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update</a><time datetime="2023-04-04T03:24:12.000Z" title="发表于 2023-04-04 11:24:12">2023-04-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/27/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DDeepRecSys-A-System-for-Optimizing-End-To-End-At-Scale-Neural-Recommendation-Inference/" title="「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference"><img src="/img/25.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference"/></a><div class="content"><a class="title" href="/2023/03/27/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DDeepRecSys-A-System-for-Optimizing-End-To-End-At-Scale-Neural-Recommendation-Inference/" title="「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference">「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference</a><time datetime="2023-03-27T02:30:28.000Z" title="发表于 2023-03-27 10:30:28">2023-03-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By zerorains</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>