<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Hierarchical Multi-Scale Attention for Semantic Segmentation | ZeroRains Blog</title><meta name="author" content="zerorains,zerorainssakura@qq.com"><meta name="copyright" content="zerorains"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Hierarchical Multi-Scale Attention for Semantic Segmentation  论文名称：Hierarchical Multi-Scale Attention for Semantic Segmentation 作者：Andrew Tao, Karan Sapra, Bryan Catanzaro 期刊：尚未查出（时间2020） 代码：https:&#x2F;&#x2F;g">
<meta property="og:type" content="article">
<meta property="og:title" content="Hierarchical Multi-Scale Attention for Semantic Segmentation">
<meta property="og:url" content="http://blog.zerorains.top/2021/08/17/Hierarchical-Multi-Scale-Attention-for-Semantic-Segmentation/index.html">
<meta property="og:site_name" content="ZeroRains Blog">
<meta property="og:description" content="Hierarchical Multi-Scale Attention for Semantic Segmentation  论文名称：Hierarchical Multi-Scale Attention for Semantic Segmentation 作者：Andrew Tao, Karan Sapra, Bryan Catanzaro 期刊：尚未查出（时间2020） 代码：https:&#x2F;&#x2F;g">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.zerorains.top/img/2.png">
<meta property="article:published_time" content="2021-08-17T02:24:19.000Z">
<meta property="article:modified_time" content="2022-06-30T08:17:58.115Z">
<meta property="article:author" content="zerorains">
<meta property="article:tag" content="语义分割">
<meta property="article:tag" content="层次注意力">
<meta property="article:tag" content="SOTA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.zerorains.top/img/2.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hierarchical Multi-Scale Attention for Semantic Segmentation",
  "url": "http://blog.zerorains.top/2021/08/17/Hierarchical-Multi-Scale-Attention-for-Semantic-Segmentation/",
  "image": "http://blog.zerorains.top/img/2.png",
  "datePublished": "2021-08-17T02:24:19.000Z",
  "dateModified": "2022-06-30T08:17:58.115Z",
  "author": [
    {
      "@type": "Person",
      "name": "zerorains",
      "url": "http://blog.zerorains.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/assets/favicon.ico"><link rel="canonical" href="http://blog.zerorains.top/2021/08/17/Hierarchical-Multi-Scale-Attention-for-Semantic-Segmentation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hierarchical Multi-Scale Attention for Semantic Segmentation',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg" style="background-image: url(/img/body_background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/assets/apple-touch-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">104</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/drink/"><i class="fa-fw fas fa-mug-hot"></i><span> 请我喝茶</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://ml.akasaki.space/"><i class="fa-fw fas fa-link"></i><span> DL笔记</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://notebook.therainisme.com/"><i class="fa-fw fas fa-link"></i><span> 急救箱</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/2.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ZeroRains Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Hierarchical Multi-Scale Attention for Semantic Segmentation</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/drink/"><i class="fa-fw fas fa-mug-hot"></i><span> 请我喝茶</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://ml.akasaki.space/"><i class="fa-fw fas fa-link"></i><span> DL笔记</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://notebook.therainisme.com/"><i class="fa-fw fas fa-link"></i><span> 急救箱</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Hierarchical Multi-Scale Attention for Semantic Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-17T02:24:19.000Z" title="发表于 2021-08-17 10:24:19">2021-08-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-06-30T08:17:58.115Z" title="更新于 2022-06-30 16:17:58">2022-06-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="hierarchical-multi-scale-attention-for-semantic-segmentation">Hierarchical Multi-Scale Attention for Semantic Segmentation</h1>
<blockquote>
<p>论文名称：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.10821">Hierarchical Multi-Scale Attention for Semantic Segmentation</a></p>
<p>作者：<a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tao%2C+A">Andrew Tao</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sapra%2C+K">Karan Sapra</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Catanzaro%2C+B">Bryan Catanzaro</a></p>
<p>期刊：尚未查出（时间2020）</p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/semantic-segmentation">https://github.com/NVIDIA/semantic-segmentation</a></p>
</blockquote>
<h2 id="原文摘要">原文摘要</h2>
<blockquote>
<p>Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test)</p>
</blockquote>
<h2 id="介绍">介绍</h2>
<p>语义分割的预测结果通常容易受到推理图像尺度的影响，如下图所示</p>
<p><img src="https://blog.zerorains.top/img/20210817104814image-20210817104813339.png" alt="image-20210817104813339"></p>
<p>在第一行中我们可以看到在图片尺度为0.5x(缩放到一半)的尺度下，路障的柱子的粗细分割结果显然不一致，但是在2.0x(放大两倍)的尺度下，路障的分割结果显然更好一些。</p>
<p>在第二行中我们可以看到在图片尺度为0.5x尺度下，道路的分割结果明显较好，但是在2.0x的尺度下，道路的分割结果就会多出一部分的内容。</p>
<p>本文将这种问题称为类混淆(class confusion)，使用多尺度推理(multi-scale inference)，将结果使用平均池化或最大池化联合起来，使得预测结果在一定范围内是有效的。使用平均值组联合多尺度通常能够促进分割结果，但是其面临将最差的分割结果与最好的分割结果结合的问题。最好的分割结果通常是使用不同尺度结果的加权组合。为了增加数据集中的方差，从而提高泛化能力，本文采用了一种自动在粗糙图像上标记的策略。与软标签策略相反，本文采用的是硬标签，以便管理标签的存储大小，这有助于通过降低磁盘IO成本，从而提高训练吞吐量。</p>
<p><strong>主要贡献</strong>：</p>
<ol>
<li>一种有效的分层多尺度注意力机制，有茱萸解决类别混淆和细节问题</li>
<li>一种基于硬阈值的i自动标签策略，利用未标记的图像并提高IOU</li>
</ol>
<h2 id="分层多尺度注意力-hierarchical-multi-scale-attention">分层多尺度注意力(Hierarchical multi-scale attention)</h2>
<p>本文的注意力机制将从每一个维度学习密集的mask，这些多尺度的预测结果，h泽泻多尺度预测通过在mask之间进行像素a形成，然后在不同尺度之间进行像素求和来组合，以获取最终的结果。</p>
<p>在hierarchical方法中，不是为一个固定的尺度集合学习所有的注意力掩码(attention mask)，而是学习向量尺度之间的相对注意力掩码。当训练网络是，只用相邻的尺度对进行训练。在训练中使用了图像缩放的数据增强方法，这允许网络学习预测一系列尺度图片的相对注意力，当进行推理时，可以层次分明地应用学习到的注意力，将N个尺度的预测结果结合在一起。在本文中，优先考虑低尺度的预测,然后逐步上升到高u尺度的预测，因为他们有更多的全局背景，可以选择那些预测需要由高尺度的预测来完善。</p>
<p>该注意力可以用如下公式来表示</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">L</mi><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi mathvariant="script">U</mi><mrow><mo fence="true">(</mo><msub><mi mathvariant="script">L</mi><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>=</mo><mn>0.5</mn><mo stretchy="false">)</mo></mrow></msub><mo>∗</mo><msub><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>=</mo><mn>0.5</mn><mo stretchy="false">)</mo></mrow></msub><mo fence="true">)</mo></mrow><mo>+</mo><mrow><mo fence="true">(</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>−</mo><mi mathvariant="script">U</mi><mrow><mo fence="true">(</mo><msub><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>=</mo><mn>0.5</mn><mo stretchy="false">)</mo></mrow></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo>∗</mo><msub><mi mathvariant="script">L</mi><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{(r=1)}=\mathcal{U}\left(\mathcal{L}_{(r=0.5)} * \alpha_{(r=0.5)}\right)+\left(\left(1-\mathcal{U}\left(\alpha_{(r=0.5)}\right)\right) * \mathcal{L}_{(r=1)}\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mrel mtight">=</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2052em;vertical-align:-0.3552em;"></span><span class="mord mathcal" style="margin-right:0.09931em;">U</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mrel mtight">=</span><span class="mord mtight">0.5</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mrel mtight">=</span><span class="mord mtight">0.5</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2052em;vertical-align:-0.3552em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathcal" style="margin-right:0.09931em;">U</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mrel mtight">=</span><span class="mord mtight">0.5</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mrel mtight">=</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></p>
<p>其中r = 1表示对图像不做任何操作，r=0.5表示下采样到原图的0.5倍，r=2上采样到原图的2倍，u表示双线性上采样操作，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">×</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo></mrow><annotation encoding="application/x-tex">+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">+</span></span></span></span>表示像素级别的乘法和加法，向网络主干中传递<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">r=0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.5</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">r=1.0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.0</span></span></span></span>两张图片，并由此产生每个尺度的语义逻辑(semantic logits)<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">L</span></span></span></span>和注意力掩码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>，这两个用于生成最终的两个尺度之间的逻辑值(logits)<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">L</span></span></span></span></p>
<p><img src="https://blog.zerorains.top/img/20210818113751image-20210818113750576.png" alt="image-20210818113750576"></p>
<p>图中的Explicit方法出自论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.03339v2">Attention to scale: Scale-aware semanticimage segmentation</a>，其主要过程就是明确学习每个尺度的注意力，在Hierarchical方法中右上角是训练方式，网络通过右上角的结构学习预测相邻尺度之间的注意力，右下角以连锁/分级（Chained/Hierarchical）的方式进行推理，以结合多个尺度的预测。低尺度的注意力决定了下一个高尺度的注意力的贡献。</p>
<p>使用分层注意力机制有两个优势：</p>
<ol>
<li>在推理时，可以灵活地选择尺度，因此在用0.5x和1.0x训练的模型上增加新的尺度0.25x和2.0x，是可以通过之前提到的注意力机制链以分层的方式进行的，而在以前的方法中只限制与使用模型训练过哼中使用的相同比例。</li>
<li>与explicit方法相比，这种分层结构使我们能够提高训练效率，在explicit方法中，如果使用0.5,1.0,2.0的尺度，训练成本为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>0.5</mn><mn>2</mn></msup><mo>+</mo><msup><mn>1.0</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2.0</mn><mn>2</mn></msup><mo>=</mo><mn>5.25</mn></mrow><annotation encoding="application/x-tex">0.5^{2}+1.0^{2}+2.0^{2}=5.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">0.</span><span class="mord"><span class="mord">5</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">1.</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">2.</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5.25</span></span></span></span>，相对与但尺度训练，使用本文的分层方法，训练成本只有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>0.5</mn><mn>2</mn></msup><mo>+</mo><msup><mn>1.0</mn><mn>2</mn></msup><mo>=</mo><mn>1.25</mn></mrow><annotation encoding="application/x-tex">0.5^{2}+1.0^{2}=1.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">0.</span><span class="mord"><span class="mord">5</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1.</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.25</span></span></span></span>​</li>
</ol>
<h3 id="结构-architecture">结构(Architecture)</h3>
<ol>
<li><strong>主干网络</strong>(Backbone)：在进行消融实验的过程中，本文使用的是ResNet-50(配置为输出步长等于8)作为算法的网络主干。对于SOTA的结果，本文使用更大，更强的主干HRNet-OCR(出自2019年的论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11065v4">Object-contextual representations for semantic segmentation</a>)</li>
<li><strong>语义头</strong>(Semantic Head)：语义预测是由一个全卷积头进行的，其构成为：3x3Conv-&gt;BN-&gt;ReLU-&gt;3x3Conv-&gt;BN-&gt;ReLU-&gt;1x1Conv，最后一层卷积输出的通道数为数据集的类别数</li>
<li><strong>注意力头</strong>(Attention Head)：注意力预测是通过一个单独的头来完成的，该头在结构上与语义头相同，除了最后的卷积输出，他输出的是一个单通道的特征图。当使用ResNet-50作为主干时，语义头和注意力头的输入都是ResNet-50的最后阶段的特征图，当使用HRNet-OCR时，语义头和注意力头的输入是OCR块的特征图。在HRNetOCR中，还存在一个辅助语义头，他在OCR之前直接从HRNet主干中获取其特征。注意力头的结构可以表示为：1x1Conv-&gt;BN-&gt;ReLU-&gt;1x1Conv，在对语义逻辑生成注意力后，预测结果被双线性上采样至目标图像的尺寸。</li>
</ol>
<h2 id="cityscapes上的自动标记-auto-labelling-on-cityscapes">Cityscapes上的自动标记(Auto Labelling on Cityscapes)</h2>
<p>受到关于图像分类任务自动标记工作的启发，本文在Cityscapes上采用了自动标记策略，以提高有效数据集的规模和标记质量。在Cityscpaes中，用20000张粗略标记的图像，以及3500张精细标记的图像。粗略图像的标签质量非常一般，并且包含了大量的未标记元素。但是经过本文的自动标签方法，可以提高标签的质量，这反过来又有助于模型的IOU指标。如下图所示，左边表示原图，中间是粗糙的标记，右边是自动标记优化后的结果</p>
<p><img src="https://blog.zerorains.top/img/20210818141216image-20210818141215561.png" alt="image-20210818141215561"></p>
<p>在本文中，采用了硬标签策略(hard labelling strategy)，即对于一个给定的像素，我们选择有teacher网络预测出的最高概率的类别。根据teacher网络的输出概率来确定标签的阈值。超过阈值的teacher网络预测结果将成为真正的标签，否则该像素被标记为ignore class，在实际使用过程中，本文使用的阈值为0.9</p>
<h2 id="实现细节">实现细节</h2>
<h3 id="训练细节">训练细节</h3>
<p>在Nvidia DGX服务器上使用pytorch进行训练，使用SGD作为优化其，在每个GPU上的BatchSize为1，momentum为0.9，权重衰减(weight decay)为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><msup><mi>e</mi><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5e^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">5</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span>。采用多项式(polynomial)学习率策略。使用RMI(出自论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.12037">Region mutual information loss for semantic segmentation</a>)作为默认设置下的主要损失函数,并使用交叉熵作为辅助损失函数，对于Cityscapes，使用2.0的poly指数，初始学习率为0.01，在2DGX节点上训练了175个epoch。对于Mapillary数据集，使用1.0的poly指数，0.02的初始学习率，并在4个DGXn解嗲上训练了200个脉冲。在数据加载其中使用类的统一采用，从每个类中频均采样，这有助于在数据分布不均的情况下改善结果。</p>
<h3 id="数据增强">数据增强</h3>
<p>在输入图像上采用高斯模糊(gaussian blur)，颜色增强(color augmentation)，随机水平翻转(random horizontal flip)和随机缩放(random scaling)（0.5x~2.0x）来增加训练过程中的数据集。对于cityscapes数据集，使用2048x1024的裁剪尺寸，对于Mapillary使用1856x1024的裁剪尺寸</p>
<h2 id="cityscpaes结果">cityscpaes结果</h2>
<p><img src="https://blog.zerorains.top/img/20210818145116image-20210818145114756.png" alt="image-20210818145114756"></p>
<h2 id="mapillary-vistas结果">Mapillary Vistas结果</h2>
<p><img src="https://blog.zerorains.top/img/20210818145138image-20210818145135609.png" alt="image-20210818145135609"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://blog.zerorains.top">zerorains</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://blog.zerorains.top/2021/08/17/Hierarchical-Multi-Scale-Attention-for-Semantic-Segmentation/">http://blog.zerorains.top/2021/08/17/Hierarchical-Multi-Scale-Attention-for-Semantic-Segmentation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://blog.zerorains.top" target="_blank">ZeroRains Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">语义分割</a><a class="post-meta__tags" href="/tags/%E5%B1%82%E6%AC%A1%E6%B3%A8%E6%84%8F%E5%8A%9B/">层次注意力</a><a class="post-meta__tags" href="/tags/SOTA/">SOTA</a></div><div class="post-share"><div class="social-share" data-image="/img/2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2021/08/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/" title="计算机网络（三）——数据链路层"><img class="cover" src="/img/27.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">计算机网络（三）——数据链路层</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2021/08/18/Object-Contextual-Representations-for-Semantic-Segmentation/" title="Object-Contextual Representations for Semantic Segmentation"><img class="cover" src="/img/32.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Object-Contextual Representations for Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1">Object-Contextual Representations for Semantic Segmentation  论文名称：Object-Contextual Representations for Semantic Segmentation 作者：Yuhui Yuan, Xilin Chen, Jingdong Wang 期刊：ECCV2020 代码：https://github.com/HRNet/HRNet-Semantic-Segmentation  原文摘要  In this paper, we study the context aggregation problem in semantic  segmentation.  Motivated  by  that  the  label  of  a  pixel  is  the category  of  the  object  that  the  pixel  belongs  to,  we  present  a  simple yet  effective  approach, ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2021/08/18/Object-Contextual-Representations-for-Semantic-Segmentation/" title="Object-Contextual Representations for Semantic Segmentation"><img class="cover" src="/img/32.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-18</div><div class="info-item-2">Object-Contextual Representations for Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1">Object-Contextual Representations for Semantic Segmentation  论文名称：Object-Contextual Representations for Semantic Segmentation 作者：Yuhui Yuan, Xilin Chen, Jingdong Wang 期刊：ECCV2020 代码：https://github.com/HRNet/HRNet-Semantic-Segmentation  原文摘要  In this paper, we study the context aggregation problem in semantic  segmentation.  Motivated  by  that  the  label  of  a  pixel  is  the category  of  the  object  that  the  pixel  belongs  to,  we  present  a  simple yet  effective  approach, ...</div></div></div></a><a class="pagination-related" href="/2021/09/03/FaPN-Feature-aligned-Pyramid-Network-for-Dense-Image-Prediction/" title="FaPN-Feature-aligned Pyramid Network for Dense Image Prediction"><img class="cover" src="/img/6.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-03</div><div class="info-item-2">FaPN-Feature-aligned Pyramid Network for Dense Image Prediction</div></div><div class="info-2"><div class="info-item-1">FaPN: Feature-aligned Pyramid Network for Dense Image Prediction  论文：FaPN: Feature-aligned Pyramid Network for Dense Image Prediction 作者：Shihua Huang, Zhichao Lu, Ran Cheng, Cheng He 期刊： ICCV2021 代码：https://github.com/EMI-Group/FaPN  原文摘要  Recent  advancements  in  deep  neural  networks  havemade remarkable leap-forwards in dense image prediction.However,  the  issue  of  feature  alignment  remains  as  ne-glected by most existing approaches for simplicity.  Directpixel addition between...</div></div></div></a><a class="pagination-related" href="/2021/09/01/Global-Aggregation-then-Local-Distribution-in-Fully-Convolutional-Networks/" title="Global Aggregation then Local Distribution in Fully Convolutional Networks"><img class="cover" src="/img/27.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-01</div><div class="info-item-2">Global Aggregation then Local Distribution in Fully Convolutional Networks</div></div><div class="info-2"><div class="info-item-1">Global Aggregation then Local Distribution in Fully Convolutional Networks  论文：Global Aggregation then Local Distribution in Fully Convolutional Networks 作者：Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong 期刊：BMVC 2019 代码：https://github.com/lxtGH/GALD-DGCNet  原文摘要  It has been widely proven that modelling long-range dependencies in fully convolutionalnetworks (FCNs) via global aggregation modules is critical for complex scene under-standing  tasks  such  as  semantic...</div></div></div></a><a class="pagination-related" href="/2021/09/11/Unifying-Nonlocal-Blocks-for-Neural-Networks/" title="Unifying Nonlocal Blocks for Neural Networks"><img class="cover" src="/img/8.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-11</div><div class="info-item-2">Unifying Nonlocal Blocks for Neural Networks</div></div><div class="info-2"><div class="info-item-1">Unifying Nonlocal Blocks for Neural Networks  论文名称：Unifying Nonlocal Blocks for Neural Networks 作者：Lei Zhu, Qi She, Duo Li, Yanye Lu, Xuejing Kang, Jie Hu, Changhu Wang 期刊：ICCV2021 代码：https://github.com/zh460045050/SNL_ICCV2021  原文摘要  The nonlocal-based blocks are designed for capturinglong-range spatial-temporal dependencies in computer vi-sion tasks. Although having shown excellent performance,they  still  lack  the  mechanism  to  encode  the  rich,  struc-tured information among elements...</div></div></div></a><a class="pagination-related" href="/2021/08/25/Region-Mutual-Information-Loss-for-Semantic-Segmentation/" title="Region Mutual Information Loss for Semantic Segmentation"><img class="cover" src="/img/9.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-25</div><div class="info-item-2">Region Mutual Information Loss for Semantic Segmentation</div></div><div class="info-2"><div class="info-item-1">Region Mutual Information Loss for Semantic Segmentation  论文名称：Region Mutual Information Loss for Semantic Segmentation 作者：Shuai Zhao, Yang Wang, Zheng Yang, Deng Cai 期刊：未查到(2019年) 代码：https://github.com/ZJULearning/RMI  原文摘要  Semantic segmentation is a fundamental problem in computer vision.  It is con-sidered as a pixel-wise classification problem in practice, and most segmentationmodels use a pixel-wise loss as their optimization criterion. However, the pixel-wise loss ignores the...</div></div></div></a><a class="pagination-related" href="/2021/05/11/Fast-SCNN%E5%BF%AB%E9%80%9F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" title="Fast-SCNN快速语义分割"><img class="cover" src="/img/24.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-11</div><div class="info-item-2">Fast-SCNN快速语义分割</div></div><div class="info-2"><div class="info-item-1">Fast-SCNN快速语义分割  论文名:Fast-SCNN: Fast Semantic Segmentation Network 作者：Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla 代码：无github代码，只有modelarts上的baseline  摘要 主要贡献：  提出了一个有竞争性(68.0%miou)，并且能在高分辨率(1024x2048)的图片实现实时(123.5FPS)语义分割的算法Fast-SCNN. 采用了离线型DCNNs中流行的跳跃连接(skip connection)，并提出了一种浅层学习的下采样模块learning to Down-sample,以此更加快速高效地进行多分支低级特征提取。 将Fast-SCNN设计为轻量型(low capacity)，并证实了无论是使用ImageNet数据集的训练模型多训练几代，还是在添加的粗糙数据中多训练几代的结果是等效的。  DCNNs的效率 高效DCNNs（Diffusion-Convolutional Neural Networks...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/assets/apple-touch-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zerorains</div><div class="author-info-description">No matter what happens, I will do my best.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">104</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zeroRains"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="tencent://message?uin=2274033547" target="_blank" title="qq"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://github.com/zeroRains" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zerorainssakura@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/kiminoamae?spm=1000.2115.3001.5343" target="_blank" title="csdn"><i class="fab fa-cuttlefish"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">主业想做大模型推理，目前也正在努力学习中。副业做数据库中执行传统模型的推理优化。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#hierarchical-multi-scale-attention-for-semantic-segmentation"><span class="toc-number">1.</span> <span class="toc-text">Hierarchical Multi-Scale Attention for Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E6%96%87%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">原文摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B1%82%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%B3%A8%E6%84%8F%E5%8A%9B-hierarchical-multi-scale-attention"><span class="toc-number">1.3.</span> <span class="toc-text">分层多尺度注意力(Hierarchical multi-scale attention)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%84-architecture"><span class="toc-number">1.3.1.</span> <span class="toc-text">结构(Architecture)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cityscapes%E4%B8%8A%E7%9A%84%E8%87%AA%E5%8A%A8%E6%A0%87%E8%AE%B0-auto-labelling-on-cityscapes"><span class="toc-number">1.4.</span> <span class="toc-text">Cityscapes上的自动标记(Auto Labelling on Cityscapes)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">1.5.</span> <span class="toc-text">实现细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82"><span class="toc-number">1.5.1.</span> <span class="toc-text">训练细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.5.2.</span> <span class="toc-text">数据增强</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cityscpaes%E7%BB%93%E6%9E%9C"><span class="toc-number">1.6.</span> <span class="toc-text">cityscpaes结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mapillary-vistas%E7%BB%93%E6%9E%9C"><span class="toc-number">1.7.</span> <span class="toc-text">Mapillary Vistas结果</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/02/%E3%80%8C%E9%9A%8F%E7%AC%94%E5%B0%8F%E8%AE%B0%E3%80%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9A%8F%E7%AC%94/" title="「随笔小记」大模型随笔"><img src="/img/14.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「随笔小记」大模型随笔"/></a><div class="content"><a class="title" href="/2025/03/02/%E3%80%8C%E9%9A%8F%E7%AC%94%E5%B0%8F%E8%AE%B0%E3%80%8D%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9A%8F%E7%AC%94/" title="「随笔小记」大模型随笔">「随笔小记」大模型随笔</a><time datetime="2025-03-02T04:54:32.000Z" title="发表于 2025-03-02 12:54:32">2025-03-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/23/%E3%80%8C%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%8DPaddle%E7%BB%84%E5%90%88%E6%9C%BA%E5%88%B6%E8%AE%BE%E8%AE%A1/" title="「实习笔记」Paddle组合机制设计与开发"><img src="/img/20.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「实习笔记」Paddle组合机制设计与开发"/></a><div class="content"><a class="title" href="/2025/02/23/%E3%80%8C%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%8DPaddle%E7%BB%84%E5%90%88%E6%9C%BA%E5%88%B6%E8%AE%BE%E8%AE%A1/" title="「实习笔记」Paddle组合机制设计与开发">「实习笔记」Paddle组合机制设计与开发</a><time datetime="2025-02-23T07:54:26.000Z" title="发表于 2025-02-23 15:54:26">2025-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/26/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DPiPAD-Pipelined-and-Parallel-Dynamic-GNN-Training-on-GPUs/" title="「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"><img src="/img/23.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"/></a><div class="content"><a class="title" href="/2023/04/26/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DPiPAD-Pipelined-and-Parallel-Dynamic-GNN-Training-on-GPUs/" title="「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs">「论文笔记」PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs</a><time datetime="2023-04-26T05:34:14.000Z" title="发表于 2023-04-26 13:34:14">2023-04-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/04/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DEkko-A-Large-Scale-Deep-Learning-Recommender-System-with-Low-Latency-Model-Update/" title="「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update"><img src="/img/26.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update"/></a><div class="content"><a class="title" href="/2023/04/04/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DEkko-A-Large-Scale-Deep-Learning-Recommender-System-with-Low-Latency-Model-Update/" title="「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update">「论文笔记」Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update</a><time datetime="2023-04-04T03:24:12.000Z" title="发表于 2023-04-04 11:24:12">2023-04-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/27/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DDeepRecSys-A-System-for-Optimizing-End-To-End-At-Scale-Neural-Recommendation-Inference/" title="「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference"><img src="/img/31.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference"/></a><div class="content"><a class="title" href="/2023/03/27/%E3%80%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8DDeepRecSys-A-System-for-Optimizing-End-To-End-At-Scale-Neural-Recommendation-Inference/" title="「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference">「论文笔记」DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference</a><time datetime="2023-03-27T02:30:28.000Z" title="发表于 2023-03-27 10:30:28">2023-03-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By zerorains</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>